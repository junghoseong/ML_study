{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화 학습\n",
    "## 4-1 다중 슬롯머신 문제\n",
    "### 4-1-1 다중 슬롯머신 문제란?\n",
    "여러 개의 팔을 가진 slot machine에서 팔마다 코인이 나오는 확률은 정해져 있지만, 그 확률값은 미리 알 수 없을 때, 제한된 횟수 안에서 가장 많은 코인을 얻기 위해서는 어떤 순서로 팔을 선택해야 할까? <br>\n",
    "1회의 행동으로 episode가 완료되므로 이 경우 상태는 필요하지 않다. <br>\n",
    "1. 환경 : 확률에 따라 보상으로 코인을 지급\n",
    "2. 에이전트 : 정책에 따라 행동으로 팔을 선택한다.\n",
    "\n",
    "### 4-1-2 탐색과 이용\n",
    "처음에는 정보 수집을 위해 슬롯머신의 팔을 선택하고 이를 `exploration`이라 한다. <br>\n",
    "정보를 수집하면 해당 정보를 기반으로 보상이 가장 높다고 판단한 팔을 선택하고, 이를 `exploitation`이라 한다. <br>\n",
    "탐색과 이용은 서로 `trade off` 관계에 있으므로 균형을 맞추는 것이 중요하다.\n",
    "\n",
    "### 4-1-3 탐색과 이용의 균형을 잡는 방법\n",
    "- $\\epsilon$-greedy\n",
    "\n",
    "$\\epsilon$-greedy는 확률 $\\epsilon$으로 랜덤하게 행동을 선택(탐색)하고, $1-\\epsilon$의 확률로 초기 보상이 최대가 되는 행동을 선택(이용)하는 방법이다. 보통 $\\epsilon$은 0.1로 설정한다.\n",
    "\n",
    "- UCB1(Upper Confidence Bound 1)\n",
    "\n",
    "UCB1은 '성공률 + 바이어스'를 최대로 만드는 행동을 선택하는 방법이다.<br>\n",
    "'성공률'은 '해당 행동의 성공 횟수 / 해당 행동의 시행 횟수'다. <br>\n",
    "'바이어스'는 '우연에 의한 성공률의 분포 크기'로 해당 행동의 시행 횟수가 작을수록 커진다. <br>\n",
    "$$ UCB1 = \\frac{w}{n} + \\sqrt{(\\frac{2log(t)}{n})^\\frac{1}{2}} $$\n",
    "\n",
    "t : 모든 행동의 시행 횟수의 합<br>\n",
    "w : 해당 행동의 성공 횟수<br>\n",
    "n : 해당 행동의 시행 횟수\n",
    "\n",
    "### 4-1-4 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 임포트\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1-5 슬롯 게임 생성\n",
    "슬롯의 팔을 표시하는 클래스 `SlotArm`을 생성한다. 생성자 인수로 `코인이 나올 확률`을 지정하고, `draw()`로 팔을 선택한 경우의 보상을 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬롯 팔 생성\n",
    "class SlotArm():\n",
    "    # 슬롯 팟 초기화\n",
    "    def __init__(self, p):\n",
    "        self.p = p # 코인이 나올 확률\n",
    "        \n",
    "    # 팔을 선택했을 때의 보상 취득\n",
    "    def draw(self):\n",
    "        if self.p > random.random():\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1-6 $\\epsilon$-greedy 계산 처리\n",
    "$\\epsilon$-greedy 계산 처리를 수행하는 클래스 `EpsilonGreedy`를 생성한다. <br>\n",
    "생성자의 인수에 `팔의 수`를 지정하고, `select_arm()`으로 정책에 따라 팔을 선택한다. 이후 `update()`로 시행 횟수와 가치를 업데이트한다. <br>\n",
    "\n",
    "- 알고리즘 파라미터 갱신\n",
    "$\\epsilon$-greedy 알고리즘의 파라미터는 다음 순서에 따라 갱신한다.\n",
    "1. 선택한 팔의 시행 횟수 + 1\n",
    "2. 선택한 팔의 가치 갱신\n",
    "<br>\n",
    "선택한 팔의 가치(평균 보상)은 다음 수식으로 갱신한다. \n",
    "$$ V_t = \\frac{n-1}{n}V_{t-1} + \\frac{1}{n}R_t$$\n",
    "'이전 회에서의 평균 정보'와 '이번 시행의 보상'으로부터 가치를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon-greedy 계산 처리\n",
    "class EpsilonGreedy():\n",
    "    # epsilon-greedy 계산 처리 초기화\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon # 탐색하는 확률\n",
    "    \n",
    "    # 시행 횟수와 가치 초기화\n",
    "    def initialize(self, n_arms):\n",
    "        self.n = np.zeros(n_arms) # 각 팔의 시행 횟수\n",
    "        self.v = np.zeros(n_arms) # 각 팔의 가치\n",
    "        \n",
    "    # 팔 선택\n",
    "    def select_arm(self):\n",
    "        if self.epsilon > random.random():\n",
    "            # 랜덤으로 팔 선택\n",
    "            return np.random.randint(0, len(self.v))\n",
    "        else:\n",
    "            # 가치가 높은 팔 선택\n",
    "            return np.argmax(self.v)\n",
    "        \n",
    "    # 알고리즘 파라미터 갱신\n",
    "    def update(self, chosen_arm, reward, t):\n",
    "        # 선택한 팔의 시행 횟수 + 1\n",
    "        self.n[chosen_arm] += 1\n",
    "        \n",
    "        # 선택한 팔의 가치 갱신\n",
    "        n = self.n[chosen_arm]\n",
    "        v = self.v[chosen_arm]\n",
    "        self.v[chosen_arm] = ((n-1) / float(n)) * v + (1 / float(n)) * reward\n",
    "        \n",
    "    # 문자열 정보 취득\n",
    "    def label(self):\n",
    "        return 'epsilon-greedy(' + str(self.epsilon) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1-7 UCB1 계산 처리\n",
    "UCB1의 계산 처리를 수행하는 클래스 `UCB1`을 생성한다. <br>\n",
    "1. `initializer()`의 인수에 팔 수를 지정한다.\n",
    "2. `select_arm()`으로 정책에 따라 팔을 선택한다.\n",
    "3. `update()`로 시행 횟수와 가치를 갱신한다.\n",
    "\n",
    "- 알고리즘 파라미터 갱신\n",
    "UCB1 알고리즘 파라미터는 다음 순서에 따라 갱신한다.\n",
    "1. 선택한 팔의 시행 횟수 + 1\n",
    "2. 성공 시, 선택한 팔의 성공 횟수 + 1\n",
    "3. 시행 횟수가 0인 팔이 존재하는 경우 가치를 갱신하지 않음\n",
    "4. 각 팔의 가치 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCB1 알고리즘\n",
    "class UCB1():\n",
    "    # 시행 횟수, 성공 횟수 및 가치 초기화\n",
    "    def initialize(self, n_arms):\n",
    "        self.n = np.zeros(n_arms) # 각 팔의 시행 횟수\n",
    "        self.w = np.zeros(n_arms) # 각 팔의 성공 횟수\n",
    "        self.v = np.zeros(n_arms) # 각 팔의 가치\n",
    "        \n",
    "    # 팔 선택\n",
    "    def select_arm(self):\n",
    "        # n이 모두 1 이상이 되도록 팔을 선택\n",
    "        for i in range(len(self.n)):\n",
    "            if self.n[i] == 0:\n",
    "                return i\n",
    "            \n",
    "        # 가치가 높은 팔을 선택\n",
    "        return np.argmax(self.v)\n",
    "    \n",
    "    # 알고리즘의 파라미터 갱신\n",
    "    def update(self, chosen_arm, reward, t):\n",
    "        # 선택한 팔의 시행 횟수 + 1\n",
    "        self.n[chosen_arm] += 1\n",
    "        \n",
    "        # 성공 시, 선택한 팔의 성공 횟수 + 1\n",
    "        if reward == 1.0:\n",
    "            self.w[chosen_arm] += 1\n",
    "        \n",
    "        # 시행 횟수가 0인 팔이 존재하는 경우에는 가치를 갱신하지 않음\n",
    "        for i in range(len(self.n)):\n",
    "            if self.n[i] == 0:\n",
    "                return \n",
    "        \n",
    "        # 각 팔의 가치 갱신\n",
    "        for i in range(len(self.v)):\n",
    "            self.v[i] = self.w[i] / self.n[i] + (2 * math.log(t) / self.n[i]) ** 0.5\n",
    "            \n",
    "    # 문자열 정보 취득\n",
    "    def label(self):\n",
    "        return 'ucb1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1-8 시뮬레이션 실행\n",
    "`play()`로 시뮬레이션을 실행하고, '게임 횟수 중 몇 번째인지'와 '보상'의 이력을 취득한다. <br>\n",
    "결정된 게임 횟수를 몇 회 시뮬레이션할지를 인수 `num_sims`로, 결정된 게임 횟수가 인수 `num_time`이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시뮬레이션 실행\n",
    "def play(algo, arms, num_sims, num_time):\n",
    "    # 이력 준비\n",
    "    times = np.zeros(num_sims * num_time) # 게임 횟수의 차수\n",
    "    rewards = np.zeros(num_sims * num_time) # 보상\n",
    "    \n",
    "    # 시뮬레이션 횟수별 루프\n",
    "    for sim in range(num_sims):\n",
    "        algo.initialize(len(arms)) # 알고리즘 설정 초기화\n",
    "        \n",
    "        # 시뮬레이션 횟수별 루프\n",
    "        for sim in range(num_sims):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
