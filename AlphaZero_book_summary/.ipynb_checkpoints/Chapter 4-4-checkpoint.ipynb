{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4 DQN을 활용한 카트-폴\n",
    "### 4-4-1 DQN을 활용한 카트-폴\n",
    "`DQN (Deep Q-Network)`에서는 테이블 형식이 아닌 뉴럴 네트워크로 행동 가치 함수를 표현한다. 입력은 '상태', 출력은 '행동'이 되는 뉴럴 네트워크로 특정한 상태에서 특정한 행동을 선택할 확률을 추론한다. <br>\n",
    "DQN 논문은 <br>\n",
    "[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)<br>\n",
    "[Human-level control through deep reinforcement learning](https://nature.com/articles/nature14236)<br>\n",
    "에서 확인 가능하다. <br>\n",
    "<br>\n",
    "이번에는 DQN으로 `OpenAI Gym`환경 중 하나인 `Cart-Pole`을 공략한다. 막대를 쓰러뜨리지 않고 균형을 잡는 게임이다. <br>\n",
    "보상의 경우 에피소드 완료 시 190 스텝 이상이면 +1, 상태는 카트의 위치, 속도, 막대의 각도, 각속도이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-2 뉴럴 네트워크 입력과 출력\n",
    "테이블 형식에서는 행동 가치 함수가 Q 학습에서의 갱신 계산식을 통해 갱신했지만, 뉴럴 네트워크는 뉴럴 네트워크의 학습에 따라 갱신한다. <br>\n",
    "뉴럴 네트워크의 입력은 환경의 상태다. 카트-폴의 상태의 수는 4이므로, 입력 형태는 (4,)이다. <br>\n",
    "뉴럴 네트워크의 출력은 행동별 가치다. 카트-폴에서의 행동의 수는 2이므로, 출력 형태는 (2,)이다. <br>\n",
    "이 뉴럴 네트워크를 학습함에 따라 특정 상태에서 특정 행동을 선택할 때의 가치를 추론할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-3 DQN의 4가지 기반 기술\n",
    "DQN은 Q 학습에서의 행동 가치 함수를 단순히 뉴럴 네트워크로 변경한 것만이 아니라, 안정된 학습을 위해 다음의 4가지 기반 기술을 표현한다.\n",
    "\n",
    "- Experience Replay\n",
    "\n",
    "Q 학습에서는 경험(상태, 행동, 보상, 다음 상태)을 순서에 따라 학습했다. 이 방법으로는 시간적으로 상관 관계가 높은 내용을 연속해서 학습하므로 학습이 안정되지 않을 수 있다. <br>\n",
    "DQN에서는 기억에 경험을 많이 저장한 뒤, 나중에 무작위로 학습한다. 이를 `Experience Replay`라고 한다.\n",
    "\n",
    "- Fixed Target Q-Network\n",
    "\n",
    "Q 학습에서는 행동 가치 함수 자체를 이용해 행동 가치 함수를 갱신했다. 즉, 갱신 중인 뉴럴 네트워크를 사용해서 해당 뉴럴 네트워크 갱신을 위한 계산을 하게 됨에 따라 학습이 안정되지 않을 수 있다. <br>\n",
    "DQN에서는 갱신량만을 계산하는 별도의 뉴럴 네트워크를 사용해서 이 문제를 해결했다. 이를 `Fixed Target Q-Network`라고 한다. <br>\n",
    "갱신 대상이 되는 뉴럴 네트워크를 `메인 네트워크`, 갱신량을 계산하기 위한 뉴럴 네트워크를 `대상 네트워크`라고 부른다. 대상 네트워크는 과거의 메인 네트워크로 일정 간격으로 메인 네트워크의 가중치를 대상 네트워크에 덮어쓰면서 갱신한다.\n",
    "\n",
    "- Reward Clipping\n",
    "\n",
    "환경으로부터 얻는 보상은 환경에 따라 그 스케일이 다르다. 이를 보완하기 위해 모든 환경에서의 보상 스케일을 -1, 0, 1로 고정한다. 이를 `Reward Clipping`이라고 부른다. <br>\n",
    "이를 통해 환경에 관계없이 동일한 하이퍼 파라미터를 사용해 학습을 수행할 수 있다.\n",
    "\n",
    "- Huber Loss\n",
    "\n",
    "뉴럴 네트워크의 오차가 큰 경우에는 오차 함수로 평균 제곱 오차를 사용하면 출력이 너무 커져 학습이 안정되지 않을 수 있다. <br>\n",
    "DQN에서는 오차가 큰 경우에도 값이 안정되도록 `휴버 함수`를 사용한다.\n",
    "\n",
    "### 4-4-4 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 임포트\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from tensorflow.losses import huber_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-5 파라미터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 준비\n",
    "NUM_EPISODES = 500 # 에피소드 수\n",
    "MAX_STEPS = 200 # 최대 스텝 수\n",
    "GAMMA = 0.99 # 시간 할인율\n",
    "WARMUP = 10 # 초기화 시 조작하지 않는 스텝 수\n",
    "\n",
    "# 검색 파라미터\n",
    "E_START = 1.0 # epsilon 초기화\n",
    "E_STOP = 0.01 # epsilon 최종값\n",
    "E_DECAY_RATE = 0.001 # epsilon 감쇠율\n",
    "\n",
    "# 메모리 파라미터\n",
    "MEMORY_SIZE = 10000 # 경험 메모리 사이즈\n",
    "BATCH_SIZE = 32 # 배치 사이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-6 행동 평가 함수 정의\n",
    "행동 평가 함수가 되는 뉴럴 네트워크 모델을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 평가 함수 정의\n",
    "class QNetwork:\n",
    "    # 초기화\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # 모델 생성\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(16, activation='relu', input_dim=state_size))\n",
    "        self.model.add(Dense(16, activation='relu'))\n",
    "        self.model.add(Dense(16, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "        \n",
    "        # 모델 컴파일\n",
    "        self.model.compile(loss=huber_loss, optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-7 경험 메모리 정의\n",
    "`경험 메모리`는 과거의 경험(상태, 행동, 보상, 다음 상태)를 저장하는 메모리다. <br>\n",
    "매 스텝마다 add()로 경험을 추가하고, sample()로 배치 사이즈만큼의 경험을 랜덤으로 취득해서 뉴럴 네트워크의 학습을 수행한다. <br>\n",
    "경험 메모리의 경험은 deque에 저장된다. deque는 list()와 비슷한 데이터 타입이나, 인수 maxlen 이상의 엘리먼트를 추가하려는 처음부터 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경험 메모리 정의\n",
    "class Memory():\n",
    "    # 초기화\n",
    "    def __init__(self, memory_size):\n",
    "        self.buffer = deque(maxlen=memory_size)\n",
    "        \n",
    "    # 경험 추가\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    # 배치 사이즈만큼의 경험을 랜덤으로 얻음\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in idx]\n",
    "    \n",
    "    # 경험 메모리 사이즈\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-8 환경 생성\n",
    "Env는 gym.make()로 생성한다. 여기서는 카트-폴의 환경을 이용하므로 인수에 `CartPole-v0`을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 생성\n",
    "env = gym.make('CartPole-v0')\n",
    "state_size = env.observation_space.shape[0] # 행동 수\n",
    "action_size = env.action_space.n # 상태 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-9 메인 네트워크, 대상 네트워크 및 경험 메모리 생성\n",
    "앞서 정의한 Q-Network와 Memory를 이용해 메인 네트워크, 대상 네트워크, 그리고 경험 메모리를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/junghoseong/anaconda3/envs/gym/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# 메인 네트워크\n",
    "main_qn = QNetwork(state_size, action_size)\n",
    "\n",
    "# 대상 네트워크 생성\n",
    "target_qn = QNetwork(state_size, action_size)\n",
    "\n",
    "# 경험 메모리 생성\n",
    "memory = Memory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-10 학습 시작\n",
    "- 환경 초기화\n",
    "\n",
    "Env의 reset()을 이용하여 환경을 초기화한다. 반환값 상태의 형태를 모델에 전달할 데이터 타입에 맞춰 '[4](상태 수) $\\rightarrow$ [1, 4](배치 크기, 상태 수)'로 변환한다.\n",
    "\n",
    "- 에피소드 수만큼 에피소드를 반복\n",
    "\n",
    "- 대상 네트워크 갱신\n",
    "\n",
    "에피소드마다 메인 네트워크의 가중치를 대상 네트워크에 덮어쓰는 처리를 수행한다. 가중치 덮어쓰기는 메인 네트워크 model의 get_weights()와 대상 네트워크 model의 set_weights()로 수행한다.\n",
    "\n",
    "- 1 에피소드의 루프\n",
    "\n",
    "- 에피소드 완료 시 로그 표시\n",
    "\n",
    "1 에피소드 완료 후 에피소드 번호, 스텝 수, $\\epsilon$의 로그를 표시한다. 그 후, 5회 연속 성공하면 학습 완료, 그렇지 않으면 다음 에피소드를 수행하기 위해 '환경 초기화'를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드: 1, 스텝 수: 18, epsilon: 0.9823\n",
      "에피소드: 2, 스텝 수: 23, epsilon: 0.9602\n",
      "에피소드: 3, 스텝 수: 47, epsilon: 0.9166\n",
      "에피소드: 4, 스텝 수: 15, epsilon: 0.9031\n",
      "에피소드: 5, 스텝 수: 19, epsilon: 0.8863\n",
      "에피소드: 6, 스텝 수: 51, epsilon: 0.8427\n",
      "에피소드: 7, 스텝 수: 14, epsilon: 0.8311\n",
      "에피소드: 8, 스텝 수: 20, epsilon: 0.8149\n",
      "에피소드: 9, 스텝 수: 13, epsilon: 0.8045\n",
      "에피소드: 10, 스텝 수: 28, epsilon: 0.7826\n",
      "에피소드: 11, 스텝 수: 21, epsilon: 0.7665\n",
      "에피소드: 12, 스텝 수: 23, epsilon: 0.7493\n",
      "에피소드: 13, 스텝 수: 14, epsilon: 0.7390\n",
      "에피소드: 14, 스텝 수: 19, epsilon: 0.7253\n",
      "에피소드: 15, 스텝 수: 41, epsilon: 0.6966\n",
      "에피소드: 16, 스텝 수: 17, epsilon: 0.6850\n",
      "에피소드: 17, 스텝 수: 51, epsilon: 0.6514\n",
      "에피소드: 18, 스텝 수: 98, epsilon: 0.5916\n",
      "에피소드: 19, 스텝 수: 76, epsilon: 0.5490\n",
      "에피소드: 20, 스텝 수: 177, epsilon: 0.4616\n",
      "에피소드: 21, 스텝 수: 36, epsilon: 0.4456\n",
      "에피소드: 22, 스텝 수: 117, epsilon: 0.3975\n",
      "에피소드: 23, 스텝 수: 23, epsilon: 0.3887\n",
      "에피소드: 24, 스텝 수: 38, epsilon: 0.3746\n",
      "에피소드: 25, 스텝 수: 200, epsilon: 0.3085\n",
      "에피소드: 26, 스텝 수: 200, epsilon: 0.2544\n",
      "에피소드: 27, 스텝 수: 200, epsilon: 0.2101\n",
      "에피소드: 28, 스텝 수: 200, epsilon: 0.1738\n",
      "에피소드: 29, 스텝 수: 200, epsilon: 0.1441\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "\n",
    "# 환경 초기화\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "\n",
    "# 에피소드 수만큼 에피소드 반복\n",
    "total_step = 0 # 총 스텝 수\n",
    "success_count = 0 # 성공 수\n",
    "for episode in range(1, NUM_EPISODES + 1):\n",
    "    step = 0 # 스텝 수\n",
    "    \n",
    "    # 대상 네트워크 갱신\n",
    "    target_qn.model.set_weights(main_qn.model.get_weights())\n",
    "    \n",
    "    # 1 에피소드 루프\n",
    "    for _ in range(1, MAX_STEPS + 1):\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "        \n",
    "        # epsilon 감소시킴\n",
    "        epsilon = E_STOP + (E_START - E_STOP) * np.exp(-E_DECAY_RATE * total_step)\n",
    "        \n",
    "        # 랜덤하게 행동 선택\n",
    "        if epsilon > np.random.rand():\n",
    "            action = env.action_space.sample()\n",
    "        # 행동 가치 함수에 따른 행동 선택\n",
    "        else:\n",
    "            action = np.argmax(main_qn.model.predict(state)[0])\n",
    "            \n",
    "        # 행동에 맞추어 상태와 보상을 얻음\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        # 에피소드 완료 시\n",
    "        if done:\n",
    "            # 보상 지정\n",
    "            if step >= 190:\n",
    "                success_count += 1\n",
    "                reward = 1\n",
    "            else:\n",
    "                success_count = 0\n",
    "                reward = 0\n",
    "            \n",
    "            # 다음 상태에 상태 없음을 대입\n",
    "            next_state = np.zeros(state.shape)\n",
    "            \n",
    "            # 경험 추가\n",
    "            if step > WARMUP:\n",
    "                memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # 에피소드 미완료 시\n",
    "        else:\n",
    "            # 보상 지정\n",
    "            reward = 0\n",
    "            \n",
    "            # 경험 추가\n",
    "            if step > WARMUP:\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "            # 상태에 다음 상태 대입\n",
    "            state = next_state\n",
    "            \n",
    "        # 행동 평가 함수 갱신\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            # 뉴럴 네트워크의 입력과 출력 준비\n",
    "            inputs = np.zeros((BATCH_SIZE, 4)) # 입력(상태)\n",
    "            targets = np.zeros((BATCH_SIZE, 2)) # 출력(행동별 가치)\n",
    "            \n",
    "            # 배치 사이즈만큼 경험을 랜덤하게 선택\n",
    "            minibatch = memory.sample(BATCH_SIZE)\n",
    "            \n",
    "            # 뉴럴 네트워크 입력과 출력 생성\n",
    "            for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):\n",
    "                \n",
    "                # 입력 상태 지정\n",
    "                inputs[i] = state_b\n",
    "                \n",
    "                # 선택한 행동의 가치 계산\n",
    "                if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
    "                    target = reward_b + GAMMA * np.amax(target_qn.model.predict(next_state_b)[0])\n",
    "                else:\n",
    "                    target = reward_b\n",
    "                    \n",
    "                # 출력에 행동별 가치 지정\n",
    "                targets[i] = main_qn.model.predict(state_b)\n",
    "                targets[i][action_b] = target # 선택한 행동 가치\n",
    "                \n",
    "            # 행동 가치 함수 갱신\n",
    "            main_qn.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "            \n",
    "        # 에피소드 완료 시\n",
    "        if done:\n",
    "            # 에피소드 루프 이탈\n",
    "            break\n",
    "    \n",
    "    # 에피소드 완료 시 로그 표시\n",
    "    print('에피소드: {}, 스텝 수: {}, epsilon: {:.4f}'.format(episode, step, epsilon))\n",
    "    \n",
    "    # 5회 연속 성공으로 학습 완료\n",
    "    if success_count >= 5:\n",
    "        break\n",
    "        \n",
    "    # 환경 초기화\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-11 1 에피소드 루프\n",
    "1 에피소드만큼 게임 종료까지의 처리를 수행한다.\n",
    "\n",
    "- $\\epsilon$ 감소\n",
    "\n",
    "파라미터 E_STOP, E_START, E_DECAY_RATE에 맞추어 $\\epsilon$을 감소시킨다.\n",
    "\n",
    "- 무작위 또는 행동 가치 함수에 따라 행동 취득\n",
    "\n",
    "$\\epsilon$과 난수에 맞추어 랜덤으로 또는 행동 가치 함수에 따라 행동을 선택한다. 랜덤 행동은 `env.action_space.sample()`, 행동 가치 함수에 따른 행동은 `np.argmax(main_qn.model.predict(state)[0])`으로 얻는다.\n",
    "\n",
    "- 행동에 맞춰 상태와 보상을 얻음\n",
    "\n",
    "Env의 step()을 사용해 행동에 맞추어 상태와 에피소드 완료 여부를 얻는다. <br>\n",
    "카트-폴이 제공하는 reward도 취득할 수 있지만, 독자적인 보상(다음 에피소드 완료 시 보상)을 사용한다.\n",
    "\n",
    "- 에피소드 완료 시 처리\n",
    "\n",
    "에피소드 완료 시 190 스텝 이상이면 보상 및 성공 횟수에 1을 더한다. <br>\n",
    "그리고 다음 상태에 `상태 없음(0만 있는 배열)`을 대입하고, 경험 메모리에 `경험`을 추가한다. 경험은 스텝 수가 `WARMUP` 이상인 경우에만 추가한다.\n",
    "\n",
    "- 에피소드 완료 불가 시 처리\n",
    "\n",
    "에피소드가 완료되지 않는 경우, 즉 보통의 스텝 실행 시에는 보상으로 0을 지정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-12 행동 가치 함수 갱신\n",
    "경험 메모리 수가 배치 사이즈 이상인 경우, 행동 가치 함수인 메인 네트워크를 갱신한다. \n",
    "\n",
    "1. 뉴럴 네트워크 입력과 출력 준비\n",
    "\n",
    "뉴럴 네트워크의 입력 `inputs`와 출력 `targets`를 준비한다. 입력은 '상태', 형태는 '(배치 사이즈, 4)' 출력은 '행동별 가치', 형태는 '(배치 사이즈, 2)'이다.\n",
    "\n",
    "2. 배치 사이즈만큼 경험을 랜덤으로 취득\n",
    "\n",
    "3. 뉴럴 네트워크의 입력과 출력 생성\n",
    "\n",
    "얻어낸 경험으로부터 상태(state_b), 행동(action_b), 보상(reward_b), 다음 상태(next_state_b)를 1 세트씩 꺼낸다. 이 값을 이용해 뉴럴 네트워크의 입력과 출력의 내용을 생성한다. <br>\n",
    "입력에 상태를 지정하기 위해 inputs[i]에 'state_b'를 대입한다. <br>\n",
    "선택한 행동의 가치를 계산한다. 다음 행동이 존재하지 않는 경우에는 target에 'reward_b'를 대입하고, 다음 행동이 존재하는 경우에는 다음 식으로 가치를 대입한다. <br>\n",
    "$$Q(s_t, a_t) \\leftarrow R_{t+1} + gamma \\times \\max_{a}Q(s_{t+1}, a)$$\n",
    "출력에 행동 별 가치를 지정하기 위해 targets[i]에 행동 가치 함수 메인 네트워크에서 추론한 '행동별 가치'를 대입한다. 그리고 targets[i][action_b]에 앞서 계산한 선택한 행동의 가치를 대입한다.\n",
    "\n",
    "4. 행동 가치 함수 갱신\n",
    "\n",
    "생성한 뉴럴 네트워크의 입력과 출력을 사용해 행동 가치 함수 메인 네트워크를 학습시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-13 에피소드 완료 시\n",
    "### 4-4-14 학습 실행 결과\n",
    "### 4-4-15 디스플레이 설정\n",
    "로커렝서는 스텝마다 Env의 `render()`를 호출해서 다른 윈도우 화면에서 환경을 표시할 수 있다. <br>\n",
    "구글 Colab과 같이 클라우드에서 실행하는 경우에는 화면을 표시할 수 없으며, 에러가 발생한다. <br>\n",
    "Xvfb(X virtual framebuffer)는 X 윈도우 시스템의 가상 디스플레이를 만들어 주는 소프트웨어다. 이를 활용해 실제 스크린이 없는 상황에서 GUI를 이용해 프로그램을 실행할 수 있다. <br>\n",
    "`pyvirtualdisplay`는 파이썬에서 가상 디스플레이(Xvfb)를 생성하는 패키지다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디스플레이 설정 install\n",
    "#apt-get -qq -y install xvfb freeglut3-dev ffmpeg x11-utils > /dev/null\n",
    "!pip install pyglet==1.3.2\n",
    "!pip install pyopengl\n",
    "!pip install pyvirtualdisplay\n",
    "\n",
    "# 디스플레이 설정 적용\n",
    "from virtualdisplay import Display\n",
    "import os\n",
    "disp = Display(visible=0, size=(1024, 768))\n",
    "disp.start()\n",
    "os.environ['DISPLAY'] = ':' + str(disp.display) + '.' + str(disp.screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-16 애니메이션 프레임 생성\n",
    "1 에피소드만큼 게임을 실행해서 스텝 수만큼의 화면 이미지를 수집한다. Env의 `render(mode='rgb_array')`를 호출해 화면의 이미지를 취득할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyglet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8073/1594183247.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 애니메이션 프레임 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 최적 행동 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mBut\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myou\u001b[0m \u001b[0mreally\u001b[0m \u001b[0mjust\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m \u001b[0mall\u001b[0m \u001b[0mGym\u001b[0m \u001b[0mdependencies\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthink\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m'pip install -e .[all]'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'pip install gym[all]'\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    "
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "frames = [] # 애니메이션 프레임\n",
    "\n",
    "# 환경 초기화\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "\n",
    "# 1 에피소드 루프\n",
    "step = 0 # 스텝 수\n",
    "for _ in range(1, MAX_STEPS + 1):\n",
    "    step += 1\n",
    "    \n",
    "    # 애니메이션 프레임 추가\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    \n",
    "    # 최적 행동 선택\n",
    "    action = np.argmax(main_qn.model.predict(state)[0])\n",
    "    \n",
    "    # 행동에 맞추어 상황과 보상을 얻음\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    \n",
    "    # 에피소드 완료 시\n",
    "    if done:\n",
    "        # 다음 상태에서 상태 없음을 대입\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # 에피소드 루프 이탈\n",
    "        break\n",
    "    else:\n",
    "        # 상태에 다음 상태를 대입\n",
    "        state = next_state\n",
    "        \n",
    "# 에피소드 완료 시 로그 표시\n",
    "print('스텝 수 : {}'.format(step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4-17 애니메이션 프레임을 애니메이션으로 변환\n",
    "애니메이션 프레임을 애니메이션으로 변환하는 경우에는 `JSAnimation`을 사용한다. matplotlib에서 javascript 애니메이션을 생성하는 패키지다.<br>\n",
    "애니메이션을 관리하는 `FuncAnimation` 객체를 생성한다. 인수로 figure 객체, 애니메이션 정기 처리, 프레임 수 및 1 프레임의 시간을 지정한다. 객체는 plt.gcf()로 얻어낸다. <br>\n",
    "이를 변경함으로써 애니메이션 표시 내용도 변경된다. 정기 처리에서는 frames의 이미지를 순서대로 표시한다.<br>\n",
    "그리고 display_animation으로 HTML 객체를 생성하고, display()로 HTML 객체를 노트북 상에 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSAnimation 설치\n",
    "!pip install JSAnimation\n",
    "\n",
    "# 패키지 임포트\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 애니메이션 재생 정의\n",
    "plt.figure(figsize = (frames[0].shape[1] / 72,0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "patch = plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "\n",
    "# 애니메이션 정기 처리\n",
    "def animate(i):\n",
    "    patch.set_data(frames[i])\n",
    "    \n",
    "# 애니메이션 재생\n",
    "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval==50)\n",
    "HTML(anim.to_jshtml())\n",
    "\n",
    "# 애니메이션 재생\n",
    "display_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
