{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥 러닝\n",
    "## 3-2 뉴럴 네트워크를 활용한 회귀\n",
    "### 3-2-1 회귀란?\n",
    "`회귀`란 다수의 특징 데이터를 기반으로 연속값 등의 수치를 예측하는 task다. \n",
    "### 3-2-2 주택 정보 데이터셋 Boston house-prices\n",
    "미국 보스턴 시 주택의 특징과 정답 라벨인 가격을 함께 저장해 둔 데이터셋이다. 훈련 데이터 404건, 테스트 데이터 102건이 포함되었으며, 13개의 특징 항목으로 관리한다. \n",
    "### 3-2-3 패키지 임포트\n",
    "`pandas`는 데이터 분석을 수행하기 위한 패키지다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 11:27:02.090272: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-26 11:27:02.090296: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# 패키지 임포트\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-4 데이터셋 준비 및 확인\n",
    "- 데이터셋 준비\n",
    "\n",
    "`boston_housing.load_data()`를 사용해 데이터셋을 4종류의 배열에 로드한다.\n",
    "\n",
    "- 데이터셋 형태 확인\n",
    " \n",
    "훈련 데이터와 훈련 라벨은 404개, 테스트 이미지와 테스트 라벨은 102개이다. 13은 데이터셋 Boston house_prices의 특징 개수다.\n",
    " \n",
    "- 데이터셋의 데이터 & 라벨 확인\n",
    "\n",
    "`pandas`는 리스트 구조인 `Series`와 테이블 구조인 `DataFrame`이라는 구조를 제공한다. 여기서 `DataFrame`를 데이터셋의 내용와 컬럼명으로 생성한 후, head()를 호출하여 테이블 형식으로 첫 10개를 출력한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404,)\n",
      "(102, 13)\n",
      "(102,)\n",
      "[15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 준비\n",
    "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()\n",
    "\n",
    "# 데이터셋의 형태 확인\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# 데이터셋의 데이터 & 라벨 확인\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df = pd.DataFrame(train_data, columns=column_names)\n",
    "df.head()\n",
    "\n",
    "print(train_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-5 데이터셋 전처리 및 확인\n",
    "- 훈련 데이터와 훈련 라벨 셔플\n",
    "\n",
    "학습 시 비슷한 데이터를 연속해서 학습하면 바이어스가 생기기 때문에 비슷한 데이터가 가까이 모여 있는 경우 데이터를 뒤섞어 주는 것이 좋다. <br>\n",
    "`np.random.random()`으로 데이터 수만큼 연속적으로 균일하게 분포된 난수를 생성하고, `np.argsort()`를 사용해 정렬하고 싶은 인덱스 순서를 생성한 뒤, 훈련 데이터와 훈련 라벨 모두에 적용한다. \n",
    "\n",
    "- 훈련 데이터와 테스트 데이터 표준화\n",
    "\n",
    "특징 데이터를 평균 0, 분산 1인 동일한 범위로 변환시킨다. $$Y = \\frac{X-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.041758</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.028326</td>\n",
       "      <td>-0.256833</td>\n",
       "      <td>1.157888</td>\n",
       "      <td>-0.536150</td>\n",
       "      <td>0.920565</td>\n",
       "      <td>-0.965710</td>\n",
       "      <td>1.675886</td>\n",
       "      <td>1.565287</td>\n",
       "      <td>0.784476</td>\n",
       "      <td>0.448077</td>\n",
       "      <td>0.498123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.838829</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.028326</td>\n",
       "      <td>-0.256833</td>\n",
       "      <td>0.867656</td>\n",
       "      <td>-3.003323</td>\n",
       "      <td>1.110488</td>\n",
       "      <td>-1.263584</td>\n",
       "      <td>1.675886</td>\n",
       "      <td>1.565287</td>\n",
       "      <td>0.784476</td>\n",
       "      <td>0.164231</td>\n",
       "      <td>1.462852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.400441</td>\n",
       "      <td>0.906544</td>\n",
       "      <td>-1.311862</td>\n",
       "      <td>-0.256833</td>\n",
       "      <td>-0.728617</td>\n",
       "      <td>0.820865</td>\n",
       "      <td>0.046203</td>\n",
       "      <td>-0.274977</td>\n",
       "      <td>-0.280929</td>\n",
       "      <td>-1.106699</td>\n",
       "      <td>-0.034578</td>\n",
       "      <td>0.448077</td>\n",
       "      <td>-0.719174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.400648</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>0.121356</td>\n",
       "      <td>-0.256833</td>\n",
       "      <td>0.133541</td>\n",
       "      <td>-0.334432</td>\n",
       "      <td>0.422466</td>\n",
       "      <td>-0.609198</td>\n",
       "      <td>-0.971569</td>\n",
       "      <td>-0.799782</td>\n",
       "      <td>1.148500</td>\n",
       "      <td>0.448077</td>\n",
       "      <td>-0.670868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.403030</td>\n",
       "      <td>1.833318</td>\n",
       "      <td>-1.076667</td>\n",
       "      <td>-0.256833</td>\n",
       "      <td>-0.626183</td>\n",
       "      <td>0.605040</td>\n",
       "      <td>-0.451896</td>\n",
       "      <td>0.982309</td>\n",
       "      <td>-0.511142</td>\n",
       "      <td>-0.216037</td>\n",
       "      <td>-0.398602</td>\n",
       "      <td>0.448077</td>\n",
       "      <td>-0.767479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0  1.041758 -0.483615  1.028326 -0.256833  1.157888 -0.536150  0.920565   \n",
       "1  1.838829 -0.483615  1.028326 -0.256833  0.867656 -3.003323  1.110488   \n",
       "2 -0.400441  0.906544 -1.311862 -0.256833 -0.728617  0.820865  0.046203   \n",
       "3 -0.400648 -0.483615  0.121356 -0.256833  0.133541 -0.334432  0.422466   \n",
       "4 -0.403030  1.833318 -1.076667 -0.256833 -0.626183  0.605040 -0.451896   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0 -0.965710  1.675886  1.565287  0.784476  0.448077  0.498123  \n",
       "1 -1.263584  1.675886  1.565287  0.784476  0.164231  1.462852  \n",
       "2 -0.274977 -0.280929 -1.106699 -0.034578  0.448077 -0.719174  \n",
       "3 -0.609198 -0.971569 -0.799782  1.148500  0.448077 -0.670868  \n",
       "4  0.982309 -0.511142 -0.216037 -0.398602  0.448077 -0.767479  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 전처리(셔플)\n",
    "order = np.argsort(np.random.random(train_labels.shape))\n",
    "train_data = train_data[order]\n",
    "train_labels = train_labels[order]\n",
    "\n",
    "# 데이터셋 전처리(표준화)\n",
    "# ndarray의 표준편차, 평균은 std, mean 메소드로 쉽게 구할 수 있다.\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "train_data = (train_data - mean) / std\n",
    "mean = test_data.mean(axis=0)\n",
    "std = test_data.std(axis=0)\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "# 데이터셋 전처리 후 데이터 확인\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "df = pd.DataFrame(train_data, columns=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-6 모델 생성\n",
    "FC Layer 3개 간단한 모델을 생성한다.\n",
    "\n",
    "### 3-2-7 컴파일\n",
    "손실함수는 `MSE`, 최적화 함수는 `Adam`, 평가 지표는 `MAE`를 지정한다. <br>\n",
    "Mean Squared Error는 평균 제곱 오차로 불리며 실제값과 예측값의 오차 제곱을 평균한 것이다. <br>\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(f_i - y_i)^2$$<br>\n",
    "Mean Absolute Error는 평균 절대 오차라고 불리며 실제값과 예측값의 오차 절대값을 평균한 것이다. <br>\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|f_i - y_i|$$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 11:50:19.079543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-26 11:50:19.080156: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-26 11:50:19.080206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-26 11:50:19.080247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-26 11:50:19.081935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-26 11:50:19.081980: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-01-26 11:50:19.082017: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-01-26 11:50:19.082026: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-01-26 11:50:19.082301: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/junghoseong/anaconda3/envs/gym/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(13, )))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 컴파일\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-8 학습\n",
    "훈련 이미지와 훈련 라벨의 배열을 모델에 전달해서 학습을 실행한다. \n",
    "\n",
    "- EarlyStopping 준비\n",
    "\n",
    "`EarlyStopping`은 지정한만큼 epoch을 반복하는 동안 개선이 없으면 학습을 중지하는 callback이다. `callback`은 `fit()`에 인수를 지정해 1 epoch 마다 임의의 처리를 실행하는 기능이다. \n",
    "\n",
    "- 학습 실행\n",
    "\n",
    "학습 실행 시 callbacks에 EarlyStopping을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 559.2391 - mae: 21.7922 - val_loss: 558.8941 - val_mae: 21.6955\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 512.0621 - mae: 20.6967 - val_loss: 509.5970 - val_mae: 20.5158\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 458.4118 - mae: 19.3661 - val_loss: 447.5793 - val_mae: 18.9497\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 390.9999 - mae: 17.5788 - val_loss: 369.3202 - val_mae: 16.7997\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 310.1837 - mae: 15.3228 - val_loss: 281.7727 - val_mae: 14.0845\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 221.8291 - mae: 12.5164 - val_loss: 195.5409 - val_mae: 10.8091\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 142.8964 - mae: 9.6029 - val_loss: 130.7616 - val_mae: 8.2953\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 89.5233 - mae: 7.3319 - val_loss: 99.6881 - val_mae: 7.2398\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 63.7625 - mae: 6.1172 - val_loss: 85.8071 - val_mae: 6.6446\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 51.9343 - mae: 5.4748 - val_loss: 71.9402 - val_mae: 6.0812\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 41.5723 - mae: 4.8423 - val_loss: 60.2860 - val_mae: 5.5189\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 34.0136 - mae: 4.3199 - val_loss: 52.0668 - val_mae: 5.0582\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 29.1288 - mae: 3.9077 - val_loss: 47.3664 - val_mae: 4.7451\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 26.3767 - mae: 3.6867 - val_loss: 43.4711 - val_mae: 4.4924\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 24.4688 - mae: 3.5167 - val_loss: 40.5286 - val_mae: 4.2754\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 22.7250 - mae: 3.3731 - val_loss: 38.3675 - val_mae: 4.1338\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 21.6891 - mae: 3.2764 - val_loss: 36.7327 - val_mae: 4.0229\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 20.7603 - mae: 3.2040 - val_loss: 35.5669 - val_mae: 3.9205\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 20.0107 - mae: 3.1670 - val_loss: 33.7317 - val_mae: 3.8427\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 18.9865 - mae: 3.1053 - val_loss: 32.4520 - val_mae: 3.7797\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 18.2807 - mae: 3.0430 - val_loss: 31.7961 - val_mae: 3.6960\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 17.7560 - mae: 2.9957 - val_loss: 31.1644 - val_mae: 3.6512\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 17.1903 - mae: 2.9589 - val_loss: 30.8283 - val_mae: 3.6321\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 16.7379 - mae: 2.9254 - val_loss: 30.6322 - val_mae: 3.6232\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 16.0746 - mae: 2.8851 - val_loss: 29.4002 - val_mae: 3.5661\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.6041 - mae: 2.8717 - val_loss: 28.8020 - val_mae: 3.5063\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 15.0742 - mae: 2.8134 - val_loss: 28.3382 - val_mae: 3.4379\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.6733 - mae: 2.7636 - val_loss: 28.2991 - val_mae: 3.4098\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 14.3412 - mae: 2.7243 - val_loss: 27.5038 - val_mae: 3.3520\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 14.0213 - mae: 2.7039 - val_loss: 26.6712 - val_mae: 3.3199\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.5340 - mae: 2.6707 - val_loss: 26.5099 - val_mae: 3.3073\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 13.1478 - mae: 2.6375 - val_loss: 26.2590 - val_mae: 3.2727\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.8446 - mae: 2.6049 - val_loss: 25.9065 - val_mae: 3.2309\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 12.7165 - mae: 2.6016 - val_loss: 25.2551 - val_mae: 3.1992\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 12.2230 - mae: 2.5359 - val_loss: 25.1400 - val_mae: 3.1612\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.9232 - mae: 2.5134 - val_loss: 24.8856 - val_mae: 3.1421\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.6662 - mae: 2.4832 - val_loss: 24.4585 - val_mae: 3.1458\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.5019 - mae: 2.4792 - val_loss: 23.7865 - val_mae: 3.0852\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.2419 - mae: 2.4506 - val_loss: 23.6985 - val_mae: 3.0353\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 11.0398 - mae: 2.4164 - val_loss: 23.8494 - val_mae: 3.0305\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.8294 - mae: 2.4115 - val_loss: 23.3824 - val_mae: 3.0264\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.5307 - mae: 2.3791 - val_loss: 23.3620 - val_mae: 2.9996\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.3610 - mae: 2.3571 - val_loss: 22.9987 - val_mae: 2.9905\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.1472 - mae: 2.3287 - val_loss: 22.7787 - val_mae: 2.9650\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 10.0271 - mae: 2.3131 - val_loss: 22.5685 - val_mae: 2.9516\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.8790 - mae: 2.2971 - val_loss: 22.2741 - val_mae: 2.9256\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.7987 - mae: 2.2747 - val_loss: 22.1050 - val_mae: 2.9168\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.7110 - mae: 2.2854 - val_loss: 20.8220 - val_mae: 2.8569\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.7518 - mae: 2.3156 - val_loss: 21.1395 - val_mae: 2.8418\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.6025 - mae: 2.2973 - val_loss: 21.0609 - val_mae: 2.8542\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.2192 - mae: 2.2306 - val_loss: 21.2467 - val_mae: 2.8598\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 9.1035 - mae: 2.2155 - val_loss: 21.3321 - val_mae: 2.8764\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.9986 - mae: 2.2010 - val_loss: 21.1084 - val_mae: 2.8760\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.0237 - mae: 2.2123 - val_loss: 20.9962 - val_mae: 2.8815\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.7348 - mae: 2.1688 - val_loss: 21.2056 - val_mae: 2.8942\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.7109 - mae: 2.1718 - val_loss: 20.8596 - val_mae: 2.8808\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.6457 - mae: 2.1658 - val_loss: 20.7080 - val_mae: 2.8684\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.5986 - mae: 2.1549 - val_loss: 20.5190 - val_mae: 2.8504\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 8.5848 - mae: 2.1556 - val_loss: 20.4856 - val_mae: 2.8381\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3803 - mae: 2.1242 - val_loss: 20.5482 - val_mae: 2.8446\n",
      "Epoch 61/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.3251 - mae: 2.1201 - val_loss: 20.6109 - val_mae: 2.8275\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.2621 - mae: 2.1116 - val_loss: 20.7876 - val_mae: 2.8664\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1801 - mae: 2.1087 - val_loss: 20.5667 - val_mae: 2.8895\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1074 - mae: 2.1016 - val_loss: 20.4468 - val_mae: 2.8828\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.1264 - mae: 2.0896 - val_loss: 20.5629 - val_mae: 2.8568\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.9854 - mae: 2.0729 - val_loss: 20.3603 - val_mae: 2.8453\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 8.0456 - mae: 2.1024 - val_loss: 19.8615 - val_mae: 2.7974\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9122 - mae: 2.0694 - val_loss: 20.0089 - val_mae: 2.7853\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.9238 - mae: 2.0631 - val_loss: 20.0722 - val_mae: 2.8030\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7984 - mae: 2.0635 - val_loss: 19.9444 - val_mae: 2.8414\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.8791 - mae: 2.0881 - val_loss: 19.8686 - val_mae: 2.8217\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7990 - mae: 2.0725 - val_loss: 19.7710 - val_mae: 2.8002\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.7623 - mae: 2.0629 - val_loss: 20.1311 - val_mae: 2.7926\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6379 - mae: 2.0333 - val_loss: 19.6395 - val_mae: 2.7856\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5907 - mae: 2.0489 - val_loss: 19.5473 - val_mae: 2.7795\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.6083 - mae: 2.0230 - val_loss: 19.9190 - val_mae: 2.7920\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.5906 - mae: 2.0192 - val_loss: 20.0713 - val_mae: 2.8198\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4348 - mae: 2.0255 - val_loss: 19.4078 - val_mae: 2.7933\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4234 - mae: 2.0202 - val_loss: 19.7621 - val_mae: 2.7806\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4681 - mae: 2.0195 - val_loss: 19.8651 - val_mae: 2.7728\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4133 - mae: 2.0080 - val_loss: 19.4995 - val_mae: 2.7832\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.2684 - mae: 2.0136 - val_loss: 19.4213 - val_mae: 2.8023\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3235 - mae: 2.0132 - val_loss: 20.0923 - val_mae: 2.8479\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3194 - mae: 2.0071 - val_loss: 19.6311 - val_mae: 2.8476\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.1702 - mae: 2.0010 - val_loss: 19.5240 - val_mae: 2.8092\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3198 - mae: 1.9905 - val_loss: 19.7076 - val_mae: 2.7867\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1767 - mae: 1.9847 - val_loss: 19.0040 - val_mae: 2.7446\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.4931 - mae: 2.0479 - val_loss: 20.1838 - val_mae: 2.8044\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.4764 - mae: 2.0309 - val_loss: 20.1672 - val_mae: 2.7524\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.3515 - mae: 2.0008 - val_loss: 20.1538 - val_mae: 2.7594\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 7.0708 - mae: 1.9683 - val_loss: 19.8262 - val_mae: 2.7713\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0757 - mae: 1.9903 - val_loss: 19.5101 - val_mae: 2.7741\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0141 - mae: 1.9887 - val_loss: 20.4460 - val_mae: 2.8294\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.0535 - mae: 1.9705 - val_loss: 20.6702 - val_mae: 2.8377\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 7.1580 - mae: 1.9682 - val_loss: 20.8940 - val_mae: 2.8820\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.9187 - mae: 1.9618 - val_loss: 20.0236 - val_mae: 2.8493\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.8659 - mae: 1.9604 - val_loss: 19.7590 - val_mae: 2.7882\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7748 - mae: 1.9335 - val_loss: 19.7817 - val_mae: 2.7912\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7173 - mae: 1.9144 - val_loss: 19.8361 - val_mae: 2.8182\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6996 - mae: 1.9199 - val_loss: 19.7272 - val_mae: 2.8223\n",
      "Epoch 101/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6693 - mae: 1.9205 - val_loss: 19.3316 - val_mae: 2.7886\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.7319 - mae: 1.9195 - val_loss: 19.2186 - val_mae: 2.7685\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.6834 - mae: 1.9476 - val_loss: 19.1867 - val_mae: 2.7887\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5711 - mae: 1.9146 - val_loss: 19.2694 - val_mae: 2.7623\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.5558 - mae: 1.8976 - val_loss: 19.0072 - val_mae: 2.7275\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5601 - mae: 1.9124 - val_loss: 18.6728 - val_mae: 2.7080\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5097 - mae: 1.9043 - val_loss: 18.7644 - val_mae: 2.7080\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3804 - mae: 1.8794 - val_loss: 18.7501 - val_mae: 2.7001\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4262 - mae: 1.8850 - val_loss: 18.8561 - val_mae: 2.6584\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.4443 - mae: 1.8763 - val_loss: 18.7477 - val_mae: 2.6863\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3150 - mae: 1.8688 - val_loss: 18.7368 - val_mae: 2.7105\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.3017 - mae: 1.8487 - val_loss: 19.0860 - val_mae: 2.7338\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2910 - mae: 1.8469 - val_loss: 18.6163 - val_mae: 2.6929\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2387 - mae: 1.8477 - val_loss: 18.4379 - val_mae: 2.6643\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.2017 - mae: 1.8411 - val_loss: 18.5445 - val_mae: 2.6797\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1659 - mae: 1.8309 - val_loss: 18.5037 - val_mae: 2.6752\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1336 - mae: 1.8235 - val_loss: 18.5855 - val_mae: 2.7001\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1824 - mae: 1.8474 - val_loss: 18.4958 - val_mae: 2.6992\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.3658 - mae: 1.9026 - val_loss: 19.4203 - val_mae: 2.7789\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.5537 - mae: 1.8737 - val_loss: 19.7656 - val_mae: 2.7678\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 6.1622 - mae: 1.8342 - val_loss: 18.6826 - val_mae: 2.6875\n",
      "Epoch 122/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1288 - mae: 1.8250 - val_loss: 18.4171 - val_mae: 2.6681\n",
      "Epoch 123/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1173 - mae: 1.8215 - val_loss: 18.7039 - val_mae: 2.7025\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9648 - mae: 1.7995 - val_loss: 18.4147 - val_mae: 2.6844\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.9819 - mae: 1.7933 - val_loss: 18.5397 - val_mae: 2.6808\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.9512 - mae: 1.7983 - val_loss: 18.0767 - val_mae: 2.6358\n",
      "Epoch 127/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9557 - mae: 1.7885 - val_loss: 18.4994 - val_mae: 2.6778\n",
      "Epoch 128/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 6.1038 - mae: 1.8470 - val_loss: 18.1737 - val_mae: 2.6563\n",
      "Epoch 129/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7522 - mae: 1.7745 - val_loss: 18.7118 - val_mae: 2.7296\n",
      "Epoch 130/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.8824 - mae: 1.7912 - val_loss: 18.5586 - val_mae: 2.6945\n",
      "Epoch 131/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7342 - mae: 1.7777 - val_loss: 18.2706 - val_mae: 2.6711\n",
      "Epoch 132/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.7701 - mae: 1.7679 - val_loss: 18.4578 - val_mae: 2.6472\n",
      "Epoch 133/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.6951 - mae: 1.7485 - val_loss: 18.4623 - val_mae: 2.6715\n",
      "Epoch 134/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6905 - mae: 1.7677 - val_loss: 18.3368 - val_mae: 2.6673\n",
      "Epoch 135/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.6977 - mae: 1.7563 - val_loss: 18.7777 - val_mae: 2.7451\n",
      "Epoch 136/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5986 - mae: 1.7465 - val_loss: 18.3943 - val_mae: 2.7035\n",
      "Epoch 137/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.6756 - mae: 1.7718 - val_loss: 18.5449 - val_mae: 2.7063\n",
      "Epoch 138/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5837 - mae: 1.7520 - val_loss: 18.0433 - val_mae: 2.6432\n",
      "Epoch 139/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.9721 - mae: 1.7862 - val_loss: 19.1225 - val_mae: 2.7659\n",
      "Epoch 140/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5634 - mae: 1.7238 - val_loss: 18.3784 - val_mae: 2.7157\n",
      "Epoch 141/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.6600 - mae: 1.7928 - val_loss: 18.3529 - val_mae: 2.7046\n",
      "Epoch 142/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5699 - mae: 1.7373 - val_loss: 18.5777 - val_mae: 2.7211\n",
      "Epoch 143/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.5178 - mae: 1.7489 - val_loss: 18.1161 - val_mae: 2.6798\n",
      "Epoch 144/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.5454 - mae: 1.7556 - val_loss: 18.4612 - val_mae: 2.7315\n",
      "Epoch 145/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4436 - mae: 1.7117 - val_loss: 18.0129 - val_mae: 2.6524\n",
      "Epoch 146/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.4122 - mae: 1.7288 - val_loss: 17.3144 - val_mae: 2.5992\n",
      "Epoch 147/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4358 - mae: 1.7300 - val_loss: 17.6885 - val_mae: 2.6392\n",
      "Epoch 148/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.4494 - mae: 1.7133 - val_loss: 17.9830 - val_mae: 2.6678\n",
      "Epoch 149/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2998 - mae: 1.7112 - val_loss: 17.6580 - val_mae: 2.6473\n",
      "Epoch 150/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2932 - mae: 1.7193 - val_loss: 17.5201 - val_mae: 2.6155\n",
      "Epoch 151/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2080 - mae: 1.6897 - val_loss: 17.4468 - val_mae: 2.6039\n",
      "Epoch 152/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2358 - mae: 1.6905 - val_loss: 17.4988 - val_mae: 2.6035\n",
      "Epoch 153/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.3077 - mae: 1.6932 - val_loss: 17.3638 - val_mae: 2.5872\n",
      "Epoch 154/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2542 - mae: 1.6889 - val_loss: 17.5070 - val_mae: 2.6123\n",
      "Epoch 155/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2727 - mae: 1.6796 - val_loss: 17.8822 - val_mae: 2.6150\n",
      "Epoch 156/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.2822 - mae: 1.7003 - val_loss: 17.7115 - val_mae: 2.6450\n",
      "Epoch 157/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1186 - mae: 1.6899 - val_loss: 17.7686 - val_mae: 2.6557\n",
      "Epoch 158/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0912 - mae: 1.6706 - val_loss: 17.7463 - val_mae: 2.6385\n",
      "Epoch 159/500\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 5.1420 - mae: 1.6761 - val_loss: 17.9832 - val_mae: 2.6873\n",
      "Epoch 160/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0961 - mae: 1.6823 - val_loss: 17.6094 - val_mae: 2.6492\n",
      "Epoch 161/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1980 - mae: 1.7138 - val_loss: 17.3958 - val_mae: 2.6116\n",
      "Epoch 162/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0610 - mae: 1.6674 - val_loss: 17.5111 - val_mae: 2.6345\n",
      "Epoch 163/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 4.9886 - mae: 1.6584 - val_loss: 17.5138 - val_mae: 2.6248\n",
      "Epoch 164/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.0578 - mae: 1.6534 - val_loss: 17.8117 - val_mae: 2.6497\n",
      "Epoch 165/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1405 - mae: 1.6847 - val_loss: 17.5489 - val_mae: 2.6283\n",
      "Epoch 166/500\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 5.1484 - mae: 1.7168 - val_loss: 17.4921 - val_mae: 2.5843\n"
     ]
    }
   ],
   "source": [
    "# EarlyStopping 준비\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20) # 20번 시행하는 동안 오차의 개선이 보이지 않는 경우 학습을 종료한다.\n",
    "\n",
    "# 학습\n",
    "history = model.fit(train_data, train_labels, epochs=500, validation_split=0.2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-9 그래프 표시 & 3-2-10 평가\n",
    "\n",
    "`fit()`의 반환값 `history`의 정보를 그래프로 표시한다. <br>\n",
    "테스트 이미지와 테스트 라벨 배열을 모델에 전달해 평가를 실행하고, 평균 절대 오차를 얻는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 638us/step - loss: 19.9472 - mae: 2.9349\n",
      "loss: 19.947\n",
      "mae: 2.935\n"
     ]
    }
   ],
   "source": [
    "# 그래프 표시\n",
    "plt.plot(history.history['mae'], label='train mae')\n",
    "plt.plot(history.history['val_mae'], label='val mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mae [1000$]')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim([0, 5])\n",
    "plt.show()\n",
    "\n",
    "# 평가\n",
    "test_loss, test_mae = model.evaluate(test_data, test_labels)\n",
    "print('loss: {:.3f}\\nmae: {:.3f}'.format(test_loss, test_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-11 추론\n",
    "테스트 데이터 중 첫 10개로 추론을 수행해서 예측 결과를 얻는다. 출력을 `flatten()`을 이용하여 2차원에서 1차원 배열로 변환한 뒤 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7. 19. 19. 27. 22. 24. 31. 23. 20. 23.]\n",
      "[ 6. 19. 24. 30. 27. 20. 28. 24. 20. 21.]\n"
     ]
    }
   ],
   "source": [
    "# 실제 가격 표시\n",
    "print(np.round(test_labels[0:10]))\n",
    "\n",
    "# 추론한 가격 표시\n",
    "test_predictions = model.predict(test_data[0:10]).flatten()\n",
    "print(np.round(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
