{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 4-3 Sarsa와 Q 학습을 활용한 미로 게임\n",
        "### 4-3-1 Sarsa와 Q 학습으로 미로 게임 풀기\n",
        "'가치 반복법'은 어떤 행동을 선택할 때 다음 상태 가치와 현재 상태 가치의 차이를 계산하고, 그 차이만큼 현재 상태를 증가시키는 방법이다. <br>\n",
        "대표적으로 `SARSA`와 `Q-Learning`이 있다. Sarsa는 수렴이 느리지만 국소적인 해답에 갇히지 않으며, Q-Learning은 수렴이 빠른 반면 국소적인 해답에 갇히기 쉽다.\n",
        "\n",
        "### 4-3-2 수익과 할인 보상 합계\n",
        "수익(Return)은 미래의 모든 지연 보상을 포함한 보상 합계이다. \n",
        "$$Return=R_{t+1}+R_{t+2}+R_{t+3}+...$$\n",
        "보상은 환경에서 얻는 것에 비해 수익은 최대화하고자 하는 목표로써 에이전트가 스스로 설정한다.\n",
        "\n",
        "할인 보상 합계(discounted return)은 자주 활용되는 예시다.\n",
        "$$Discounted_Return=R_{t+1}+\\gamma \\times R_{t+2}+\\gamma^2 \\times R_{t+3}+...$$\n",
        "\n",
        "### 4-3-3 행동 가치 함수와 상태 가치 함수\n",
        "\n",
        "수익은 아직 발생하지 않은, 미래에 일어날 가능성이 있는 일을 포함한 확정되지 않은 값이기 때문에 에이전트의 상태와 정책을 고정한 뒤 조건부로 수익을 계산한다. <br>\n",
        "\n",
        "이 가치를 계산하는 함수에는 `행동 가치 함수`와 `상태 가치 함수`가 있다.\n",
        "\n",
        "- 행동 가치 함수\n",
        "\n",
        "Action Value Function은 특정한 상태에서 특정한 행동을 선택하는 가치를 계산하는 함수다. 행동 가치 함수는 기호 Q로 표시하므로 `Q 함수`라고 부른다. <br>\n",
        "\n",
        "예를 들어 `s=5, a=down`일 때, 골인 지점에 도착하여 보상 1을 받는다면 식으로 다음과 같다.\n",
        "$$Q(s=5, a=down)=R_{t+1}=1$$\n",
        "이 때, `a=left`를 선택한다면, 5->4->5->8과 같이 2개의 스텝을 밟기 위한 시간이 필요하다. 이는 식으로 다음과 같다.\n",
        "$$Q(s=5, a=left)=\\gamma^2 \\times1$$\n",
        "에이전트가 4 구역에 있을 때, 골인 지점에 이르기 위해서는 4->5->8로 이동해야 한다. 이는 식으로 다음과 같다.\n",
        "$$Q(s=4, a=right)=R_{t+1} + \\gamma \\times Q(s=5, a=down) = 0 + \\gamma \\times 1 = \\gamma$$\n",
        "\n",
        "- 상태 가치 함수\n",
        "\n",
        "State Value Function은 특정 상태의 가치를 계산하는 함수이다. 기호 V로 표현한다.<br>\n",
        "예를 들어 `s=5, a=down`을 선택하면 보상 1을 받을 수 있다. 이는 식으로 다음과 같다.\n",
        "$$V(s=5)=R_{t+1}=1$$\n",
        "에이전트가 4 구역에 있다면 4->5->8로 이동해야 하므로 식으로 다음과 같다.\n",
        "$$V(s=4)=R_{t+1} + \\gamma \\times V(s=5)=0 + \\gamma \\times 1 = \\gamma$$\n",
        "\n",
        "### 4-3-4 벨만 방정식과 마르코프 결정 과정\n",
        "Bellman Equation은 상태 가치 함수와 행동 가치 함수의 수식을 일반적인 형태로 바꾸어 쓴 식이다.\n",
        "$$ Q(s_t, a_t) = R_{t+1} + \\gamma \\times Q(s_{t+1}, a_{t+1})$$\n",
        "$$ V(s_t) = R_{t+1} + \\gamma \\times V(s_{t+1})$$\n",
        "이 벨만 방정식이 성립하기 위해서는 환경이 Markov Decision Process를 따라야 한다. \n",
        "벨만 방정식으로부터 행동 가치 함수를 학습하는 방법으로 SARSA, Q-Learning 등이 있다. 상태 가치 함수로는 Dueling Network, A2C 등의 학습 알고리즘을 사용하기도 한다.\n",
        "\n",
        "### 4-3-5 가치 반복법 학습 순서\n",
        "1. 랜덤 행동 준비\n",
        "2. 행동 가치함수 준비\n",
        "3. 행동에 따라 다음 상태 얻기\n",
        "4. 랜덤 또는 행동 가치 함수에 따라 행동 얻기\n",
        "5. 행동 가치 함수 갱신\n",
        "6. 골인 지점에 이를 때 까지 3~5단계 반복\n",
        "7. 에피소드 3~6을 반복하며 학습\n",
        "\n",
        "행동 가치 함수가 충분히 학습되지 않은 상태에서 행동 가치 함수만으로 행동을 선택하면 발견하지 못한 좋은 선택을 간과할 수 있으므로 확률 $\\epsilon$에 따라 무작위로 행동을 선택할 필요가 있다.\n",
        "\n",
        "### 4-3-6 패키지 임포트"
      ],
      "metadata": {
        "id": "v7Vp8pQblzuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 임포트\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "8KM1pRlXl9I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-3-7 미로 생성\n",
        "그래프 표시 라이브러리 matplotlib을 사용하여 미로를 생성한다.\n",
        "\n",
        "### 4-3-8 랜덤 행동 준비\n",
        "랜덤한 행동은 정책 경사법과 같이 파라미터 $\\theta$, 정책을 생성하고, `np.random.choice()`로 분산을 추가해 생성한다. 단, 정책 경사법과 달리 파라미터와 정책은 갱신하지 않는다.\n",
        "\n",
        "### 4-3-9 행동에 따라 다음 상태 얻기\n",
        "\n",
        "### 4-3-10 행동 가치 함수 준비\n",
        "\n",
        "### 4-3-11 랜덤 또는 행동 가치 함수에 따라 행동 얻기\n",
        "\n",
        "### 4-3-12 Sarsa를 통한 행동 가치 함수 갱신"
      ],
      "metadata": {
        "id": "yEHKz5PnEBK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P8nQNwnbD_XQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}