{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알파제로와 머신러닝 개요\n",
    "\n",
    "## 1-1 알파고와 알파고 제로, 그리고 알파제로\n",
    "\n",
    "### 1-1-1 알파고\n",
    "> AlphaGo는 DeepMind(DQN, AlphaGo를 개발한 후 2014년에 구글에 인수됨)에서 개발한 컴퓨터 바둑 프로그램으로, 핸디캡이 주어지지 않은 상태에서 프로 바둑 기사에게 승리를 거둔 최초의 게임 AI다.\n",
    "알파고는 과거부터 사용되던 '몬테카를로 트리 탐색', '알고리즘 기반의 탐색을 활용한 예측 능력', '딥러닝을 활용한 국면에서 최선의 수를 예측하는 직감', '강화학습을 활용한 셀프 플레이에서 얻은 경험' 등을 조합하여 최강의 AI가 되었다.\n",
    "\n",
    "### 1-1-2 알파고 제로\n",
    "> AlphaGo Zero는 2017년에 프로 기사의 기보 데이터를 사용하지 않고, 백지 상태에서 셀프 플레이만으로 최선의 수를 예측하는 학습을 수행했다.\n",
    "\n",
    "### 1-1-3 알파 제로\n",
    "> AlphaZero는 바둑 뿐만 아니라, 체스나 장기까지 학습할 수 있었으며, 당시 게임 AI 챔피언이던 것들에 모두 승리를 거두었으며, 바둑판 면을 회전시켜서 학습 데이터를 부풀리는 바둑 특유의 학습 방법을 전혀 사용하지 않으며 결과에 무승부를 추가했다. 이를 통해 숙련자가 만들어 낸 데이터 없이 임의의 task를 학습할 수 있는 범용 알고리즘이 구현되었다.\n",
    "\n",
    "`<논문>` <br> AlphaZero의 논문 `<A general reinforcement learning algorithm that masters chess, shogi and Go through self-play>` <br>\n",
    "원문은 [여기](https://science.sciencemag.org/content/362/6419/1140)에서 찾아볼 수 있으며, Deepmind는 이후에 알파폴드나 알파스타와 같은 새 AI를 개발했다.\n",
    "\n",
    "`<AlphaStar>` <br>\n",
    "스타크래프트 2를 공략하는 AI로 2019년에 프로게이머를 이겼으며, '턴제 방식'인 바둑에 반해, '리얼타임'방식 게임인 스타크래프트의 선택 가능한 행동의 수는 361인 바둑에 비해, 1026개로, 복잡성이 높다.\n",
    "원문은 [여기](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii)에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 딥 러닝 개요\n",
    "### 1-2-1 딥 러닝이란?\n",
    "대량의 데이터 속에서 규칙성을 발견해 추론을 수행하기 위한 규칙을 만드는 머신러닝 방법 중 하나인 딥 러닝은 인공지능 연구 분야의 하나다.\n",
    "기존 규칙기반(rule-based)에서는 규칙과 데이터를 받아 대답을 받았다면, 머신러닝은 데이터와 대답을 받아 규칙을 도출하는 과정인 학습을 수행한다. <br>\n",
    "`딥 러닝`은 `Neural Network`를 이용해서 머신러닝을 수행하며, 뉴럴 네트워크는 네트워크 구조와 조정 가능한 가중치 파라미터로 구성된다.\n",
    "### 1-2-2 뉴런과 뉴럴 네트워크\n",
    "뉴런은 사람의 뇌 속의 신경 세포로, 임계값 이상인 값은 1, 미만인 값은 0을 출력시키며, 최적화를 수행하는 과정에서 임계값은 bias라고 부른다.\n",
    "뉴런을 나열하여 layer를 만들고 이를 쌓아 올려 복잡한 문제를 다룰 수 있는 뉴럴 네트워크를 만든다. 4 layer 이상의 깊은 뉴럴 네트워크를 딥 뉴럴 네트워크라고 부른다.\n",
    "### 1-2-3 모델 작성, 학습 및 추론\n",
    "`modeling`에서는 뉴럴 네트워크의 네트워크 구조를 작성한다. <br>\n",
    "`learning`에서는 가중치 파라미터와 바이어스를 최적화한다. 이 과정에서 `backpropagation`을 활용해 값들을 갱신한다. <br>\n",
    "`inference`에서는 테스트 데이터를 모델에 입력해 예측값을 출력한다.\n",
    "### 1-2-4 지도 학습, 비지도 학습 및 강화 학습\n",
    "`supervised learning`은 입력과 출력의 관계를 학습하며, 예측의 기초가 되는 정답 데이터와 학습에 사용되는 학습 데이터 세트로 학습을 시킨다. <br>\n",
    "`unsupervised learning`은 데이터의 구조를 학습하는 방법으로, 학습 데이터만을 사용해 학습을 수행하며, 학습 데이터에 포함된 잠재적인 패턴을 도출하는 추론 모델을 생성한다. <br>\n",
    "`reinforcement learning`은 에이전트가 환경의 상태에 맞춰 어떤 행동을 해야 보상을 가장 많이 받을 수 있는지를 구하는 방법이다. 학습 데이터가 주어지지 않은 채, 에이전트는 시행착오만을 통해 학습한다. <br>\n",
    "### 1-2-5 Convolutional Neural Network & Recurrent Neural Network\n",
    "`CNN`은 이미지 인식 분야에서 높은 성능을 발휘했으며 Convolutional layer와 pooling layer를 결합하여 사용한다. 입력 이미지의 특징을 Convolutional layer에서 유지하면서 큰 폭으로 압축하며, pooling layer에서 이미지의 국소적인 왜곡이나 평행 이동의 영향을 쉽게 받지 않도록 robustness를 확보한다. \n",
    "`RNN`은 시계열을 다루는 뉴럴 네트워크로 동영상 분류, 자연어 처리, 음성 인식 등에 이용되며, 히든 레이어에 자기 피드백을 줄 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 강화 학습 개요\n",
    "### 1-3-1 강화학습이란?\n",
    "강화학습은 에이전트가 환경의 상태에 맞춰 어떻게 행동해야 가장 많은 보상을 받을 수 있는지 찾아내는 방법이다. 지도 학습이나 비지도 학습과 달리, 학습 데이터 없이 스스로의 시행착오만으로 학습을 하는 것이 특징이다.\n",
    "### 1-3-2 강화 학습 용어\n",
    "> 에이전트와 환경 <br>\n",
    "\n",
    "강화학습에서는 행동하는 주체를 에이전트, 에이전트가 존재하는 세계를 환경이라고 부른다.\n",
    "\n",
    "> 행동과 상태\n",
    "\n",
    "에이전트가 환경에서 일으키는 움직임을 행동이라고 부른다. 에이전트의 행동에 따라 변화하는 환경적인 요소를 상태라 부른다.\n",
    "\n",
    "> 보상\n",
    "\n",
    "강화학습에서는 행동의 좋고 나쁨을 표시하는 지표로 보상을 사용한다.\n",
    "\n",
    "> 정책\n",
    "\n",
    "어떤 상황에서 어떤 행동을 수행하는 확률을 정책이라고 부른다. 강화 학습의 목적은 많은 보상을 얻을 수 있는 정책을 찾아내는 것이다.\n",
    "\n",
    "> 수익과 가치\n",
    "\n",
    "수익(interest, return), 모든 지연 보상(discounted reward)를 포함한 보상의 합을 최대화하는 것을 강화학습의 목표로 한다. 수익이 미래의 일이므로 확실하지 않기 때문에, 에이전트의 상태와 정책을 고정한 상태로 조건부 수익을 계산한다. 이를 가치(value)라고 부른다. 이 가치를 크게 만들 수 있는 조건을 찾아낸다면 학습한 것이다.\n",
    "\n",
    "> 강화학습의 요약\n",
    "\n",
    "가치를 최대화 하면 수익이 최대화되고, 최종적으로 많은 보상을 받을 수 있는 정책을 만들 수 있다.\n",
    "### 1-3-3 강화 학습의 학습 사이클\n",
    "1. 에이전트는 처음에 무엇을 해야 할지 판단할 수 없으므로 선택 가능한 행동 중에서 랜덤으로 행동을 결정한다.\n",
    "2. 에이전트는 보상을 받을 때, 어떤 상태에서 어떤 행동을 했을 때 얼마만큼의 보상을 받았는지의 경험을 기억한다.\n",
    "3. 경험을 기반으로 정책을 계산한다.\n",
    "4. 무작위로 행동하는 것을 유지하면서 정책을 단서로 행동을 결정한다.\n",
    "5. 2~4를 반복하면서 장기적으로 많은 보상을 받을 수 있는 정책을 계산한다.\n",
    "\n",
    "이 학습 사이클을 마르코프 결정과정 (Markov Decision Process, MDP)라고 부른다. 마르코프 결정과정은 현재 상태에서 선택한 행동에 따라 다음 상황이 확정되는 시스템을 말한다. 1회 행동 분량을 step, 게임이 종료될 때까지의 1회 학습 분량을 episode라고 부른다.\n",
    "\n",
    "### 1-3-4 정책 계산 방법\n",
    "강화학습에서는 크게 정책 반복법(Policy Iteration)과 가치 반복법(Value Iteration)의 두 가지로 정책을 계산한다.\n",
    "> 정책 반복법\n",
    "\n",
    "정책을 따라 이동하며, 성공 시 선택한 행동이 중요하다고 판단해 그 행동을 많이 선택하는 방식으로 정책을 갱신하는 방법으로, Policy Gradient가 가장 유명한 알고리즘이다.\n",
    "\n",
    "> 가치 반복법\n",
    "\n",
    "다음 상태 가치와 현재 상태 가치의 차이를 계산하고, 그 차이만큼만 현재 상태의 가치를 늘리는 방법이다. 'SARSA', 'Q-Learning'이 이를 이용한 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4 탐색 개요\n",
    "### 1-4-1 탐색이란?\n",
    "`탐색(Exploration)`은 현재 국면을 시작점으로 몇 수 앞까지의 전개 상황을 미리 읽어 내고, 읽어 낸 국면의 상태 평가를 기반으로 하여 현재 국면에서 가장 좋은 '다음 한 수'를 선택하는 방법이다. game tree를 사용해 모델화 하며, `국면`을 `node`로 표시하고, `수`를 `arch`로 표시한 트리 구조다. <br>\n",
    "<br>\n",
    "가장 상위 노드를 `루트 노드라`고 부르며, 이는 현재 국면을 표시한다. 가장 하위의 노드를 `리프 노드`라고 부르며, 전개 이후의 국면을 표시한다. 리프 노드에 쓰여진 숫자는 해당 국면에서 무언가의 수단을 통해 산출한 자신의 상태에 대한 평갓값이다. <br>\n",
    "<br>\n",
    "또한, 게임 트리는 노드 관계를 '가족 관계'라고 부르며 한 단계 상위의 노드는 `부모 노드`, 한 단계 하위의 노드는 `자식 노드`, 부모 노드에서 볼때 자신 이외의 자식 노드를 `형제 노드`라고 부른다. \n",
    "\n",
    "### 1-4-2 완전 게임 트리와 부분 게임 트리\n",
    "게임을 시작할 때부터 선택할 수 있는 모든 수를 포함하고 있는 게임 트리를 완전 게임트리라고 부른다. 보통 경우의 수가 많아 연산이 불가능하다. 그래서 부분 게임 트리라는 현재 국면에서 주어진 시간 내에 탐색 가능한 부분만을 포함하는 게임 트리를 이용한다. 유용하다고 판단한 노드는 가능한 한 깊이 탐색하고, 그렇지 않다고 판단한 노드는 도중에 탐색을 중단한다. 이 부분 게임 트리를 품질이 뛰어나게 효율적으로 만들어 내는가가 게임 AI의 성능을 결정한다. \n",
    "\n",
    "### 1-4-3 이 책의 내용\n",
    "MinMax Algorithm, Alpha-beta Algorithm, Monte-Carlo Method, Monte-Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
