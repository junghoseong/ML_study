{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRNet을 이용하여 적은 빛의 이미지 품질 향상\n",
    "### Introduction\n",
    "낮은 해상도의 버전에서, 높은 수준의 이미지 품질을 재현하려는 목표를 가진 이미지 복원은 여러 분야에 적용되고 있다. 예를 들면, 사진, 보안, 의료영상, 원격 탐사 등등에 사용된다. 이 예제에서는, MIRNet 모델을 구현하여 적은 빛의 이미지 품질을 향상시킬 것이다. MIRNet은 컨볼루셔널 구조로 높은 해상도의 공간적 세부사항을 보존하면서, 여러 이미지 크기에서 문맥적 정보를 합한 특징들 집합을 학습한다.\n",
    "## Downloading LOLDataset\n",
    "LOL Dataset은 적은 빛을 가진 이미지 품질을 향상시키기 위해 만들어진 데이터셋이다. 학습을 위해 485장의 이미지를, 시험을 위해 15장의 이미지를 제공한다. 데이터셋에서 각 이미지 쌍은 적은 빛의 입력 이미지와 해당되는 빛에 잘 노출된 참고 이미지로 구성된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests[socks] in /usr/lib/python3/dist-packages (from gdown) (2.22.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.6.0)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.62.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown) (1.14.0)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=c98e2f92b1de3bdec4f210bc4333e9c92d11e9af2b6a51f9bb4eca17526b66ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/7b/5d/656f46cd6889e4c93977be9586901d0adc1271b2d876c84c96\n",
      "Successfully built gdown\n",
      "Installing collected packages: soupsieve, beautifulsoup4, gdown, PySocks\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.10.0 gdown-4.4.0 soupsieve-2.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1DdGIJ4PZPlF2ikl8mNM9V-PdVxVLbQi6\n",
      "To: /data/바탕화면/ML_study/Keras Tutorial/lol_dataset.zip\n",
      "100%|████████████████████████████████████████| 347M/347M [00:07<00:00, 47.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# download dataset\n",
    "#!pip install gdown\n",
    "#!gdown https://drive.google.com/uc?id=1DdGIJ4PZPlF2ikl8mNM9V-PdVxVLbQi6\n",
    "#!unzip -q lol_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TensorFlow Dataset\n",
    "우리는 LOL 데이터셋의 학습용 집합에서 300장의 이미지 쌍을 사용할 것이고, 우리는 남은 185장의 이미지 쌍을 검증을 위해 사용할 것이다. 우리는 학습과 검증을 위해 사용될 이미지 쌍들을 128X128의 크기로 랜덤한 부분을 만들것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: <BatchDataset shapes: ((4, None, None, 3), (4, None, None, 3)), types: (tf.float32, tf.float32)>\n",
      "Val Dataset: <BatchDataset shapes: ((4, None, None, 3), (4, None, None, 3)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "IMAGE_SIZE=128\n",
    "BATCH_SIZE=4\n",
    "MAX_TRAIN_IMAGES=300\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def random_crop(low_image, enhanced_image):\n",
    "    low_image_shape = tf.shape(low_image)[:2]\n",
    "    low_w = tf.random.uniform(\n",
    "        shape = (), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    low_h = tf.random.uniform(\n",
    "        shape = (), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    enhanced_w = low_w\n",
    "    enhanced_h = low_h\n",
    "    low_image_cropped = low_image[\n",
    "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
    "    ]\n",
    "    enhanced_image_cropped = enhanced_image[\n",
    "        enhanced_h : enhanced_h + IMAGE_SIZE, enhanced_w : enhanced_w + IMAGE_SIZE\n",
    "    ]\n",
    "    return low_image_cropped, enhanced_image_cropped\n",
    "\n",
    "def load_data(low_light_image_path, enhanced_image_path):\n",
    "    low_light_image = read_image(low_light_image_path)\n",
    "    enhanced_image = read_image(enhanced_image_path)\n",
    "    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n",
    "    return low_light_image, enhanced_image\n",
    "\n",
    "def get_dataset(low_light_images, enhanced_images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
    "train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n",
    "\n",
    "val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
    "val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n",
    "\n",
    "test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n",
    "test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n",
    "\n",
    "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
    "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIRNet Model\n",
    "여기에 MIRNet 모델의 주요 특징들을 담았다. <br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_kernel_feature_fusion(\n",
    "    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n",
    "):\n",
    "    channels = list(multi_scale_feature_1.shape)[-1]\n",
    "    combined_feature = layers.Add()(\n",
    "        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n",
    "    )\n",
    "    gap = layers.GlobalAveragePooling2D()(combined_feature)\n",
    "    channel_wise_statistics = tf.reshape(gap, shape=(-1, 1, 1, channels))\n",
    "    compact_feature_representation = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(channel_wise_statistics)\n",
    "    feature_descriptor_1 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_2 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_3 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n",
    "    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n",
    "    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n",
    "    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n",
    "    return aggregated_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention_block(input_tensor):\n",
    "    average_pooling = tf.reduce_max(input_tensor, axis=-1)\n",
    "    average_pooling = tf.expand_dims(average_pooling, axis=-1)\n",
    "    max_pooling = tf.reduce_mean(input_tensor, axis=-1)\n",
    "    max_pooling = tf.expand_dims(max_pooling, axis=-1)\n",
    "    concatenated = layers.Concatenate(axis=-1)([average_pooling, max_pooling])\n",
    "    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(concatenated)\n",
    "    feature_map = tf.nn.sigmoid(feature_map)\n",
    "    return input_tensor * feature_map\n",
    "\n",
    "\n",
    "def channel_attention_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "    feature_descriptor = tf.reshape(average_pooling, shape=(-1, 1, 1, channels))\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(feature_descriptor)\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n",
    "    )(feature_activations)\n",
    "    return input_tensor * feature_activations\n",
    "\n",
    "\n",
    "def dual_attention_unit_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    feature_map = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(input_tensor)\n",
    "    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n",
    "        feature_map\n",
    "    )\n",
    "    channel_attention = channel_attention_block(feature_map)\n",
    "    spatial_attention = spatial_attention_block(feature_map)\n",
    "    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n",
    "    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n",
    "    return layers.Add()([input_tensor, concatenation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Residual Modules\n",
    "\n",
    "\n",
    "def down_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.MaxPooling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.MaxPooling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "def up_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.UpSampling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.UpSampling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "# MRB Block\n",
    "def multi_scale_residual_block(input_tensor, channels):\n",
    "    # features\n",
    "    level1 = input_tensor\n",
    "    level2 = down_sampling_module(input_tensor)\n",
    "    level3 = down_sampling_module(level2)\n",
    "    # DAU\n",
    "    level1_dau = dual_attention_unit_block(level1)\n",
    "    level2_dau = dual_attention_unit_block(level2)\n",
    "    level3_dau = dual_attention_unit_block(level3)\n",
    "    # SKFF\n",
    "    level1_skff = selective_kernel_feature_fusion(\n",
    "        level1_dau,\n",
    "        up_sampling_module(level2_dau),\n",
    "        up_sampling_module(up_sampling_module(level3_dau)),\n",
    "    )\n",
    "    level2_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n",
    "    )\n",
    "    level3_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(down_sampling_module(level1_dau)),\n",
    "        down_sampling_module(level2_dau),\n",
    "        level3_dau,\n",
    "    )\n",
    "    # DAU 2\n",
    "    level1_dau_2 = dual_attention_unit_block(level1_skff)\n",
    "    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n",
    "    level3_dau_2 = up_sampling_module(\n",
    "        up_sampling_module(dual_attention_unit_block(level3_skff))\n",
    "    )\n",
    "    # SKFF 2\n",
    "    skff_ = selective_kernel_feature_fusion(level1_dau_2, level3_dau_2, level3_dau_2)\n",
    "    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n",
    "    return layers.Add()([input_tensor, conv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mAdd()([input_tensor, conv])\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mModel(input_tensor, output_tensor)\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmirnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rrg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_mrb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mmirnet_model\u001b[0;34m(num_rrg, num_mrb, channels)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmirnet_model\u001b[39m(num_rrg, num_mrb, channels):\n\u001b[0;32m---> 10\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     11\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(channels, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(input_tensor)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rrg):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "def recursive_residual_group(input_tensor, num_mrb, channels):\n",
    "    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_mrb):\n",
    "        conv1 = multi_scale_residual_block(conv1, channels)\n",
    "    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n",
    "    return layers.Add()([conv2, input_tensor])\n",
    "\n",
    "\n",
    "def mirnet_model(num_rrg, num_mrb, channels):\n",
    "    input_tensor = keras.Input(shape=[None, None, 3])\n",
    "    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_rrg):\n",
    "        x1 = recursive_residual_group(x1, num_mrb, channels)\n",
    "    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n",
    "    output_tensor = layers.Add()([input_tensor, conv])\n",
    "    return keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "\n",
    "model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charbonnier_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n",
    "\n",
    "\n",
    "def peak_signal_noise_ratio(y_true, y_pred):\n",
    "    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_peak_signal_noise_ratio\",\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            min_delta=1e-7,\n",
    "            mode=\"max\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"peak_signal_noise_ratio\"], label=\"train_psnr\")\n",
    "plt.plot(history.history[\"val_peak_signal_noise_ratio\"], label=\"val_psnr\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.title(\"Train and Validation PSNR Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(images, titles, figure_size=(12, 12)):\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(len(images)):\n",
    "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
    "        _ = plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def infer(original_image):\n",
    "    image = keras.preprocessing.image.img_to_array(original_image)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    output = model.predict(image)\n",
    "    output_image = output[0] * 255.0\n",
    "    output_image = output_image.clip(0, 255)\n",
    "    output_image = output_image.reshape(\n",
    "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
    "    )\n",
    "    output_image = Image.fromarray(np.uint8(output_image))\n",
    "    original_image = Image.fromarray(np.uint8(original_image))\n",
    "    return output_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(images, titles, figure_size=(12, 12)):\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(len(images)):\n",
    "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
    "        _ = plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def infer(original_image):\n",
    "    image = keras.preprocessing.image.img_to_array(original_image)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    output = model.predict(image)\n",
    "    output_image = output[0] * 255.0\n",
    "    output_image = output_image.clip(0, 255)\n",
    "    output_image = output_image.reshape(\n",
    "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
    "    )\n",
    "    output_image = Image.fromarray(np.uint8(output_image))\n",
    "    original_image = Image.fromarray(np.uint8(original_image))\n",
    "    return output_image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
